{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply noise-diode-based gain corrections to azimuth and raster scans\n",
    "### A. Ordog, July 2022\n",
    "### NOTE: please do not modify\n",
    "#### July 12, 2022: updated to include RR gains, set outlier threshold based on stdev, write out new files with all 4 products corrected\n",
    "#### July 18, 2022: modified to use polynomial fit to gain vs time rather than interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dva_sdhdf_combine\n",
    "import imp\n",
    "import os\n",
    "import subprocess\n",
    "import h5py\n",
    "import numpy as np\n",
    "from astropy.time import Time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import HourLocator as HourLocator\n",
    "from matplotlib.dates import MinuteLocator as MinuteLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from astropy import units as u\n",
    "from astropy.time import TimeDelta\n",
    "from scipy import interpolate\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from astropy.convolution import convolve, Box1DKernel\n",
    "\n",
    "day ='09'\n",
    "trim_rast_exist = False\n",
    "trim_az_exist = True\n",
    "\n",
    "#### Change the directory to where the files are located\" ####\n",
    "#dir_in = '/media/ordoga/DVA_data/survey_phase1_day'+day+'/'\n",
    "#dir_out = '/media/ordoga/DVA_data/survey_phase1_day'+day+'/'\n",
    "##############################################################\n",
    "\n",
    "### Use these directories on elephant: ###################\n",
    "dir_in_rast  = '/srv/data/dva/survey_raster/'\n",
    "dir_out_rast = '/srv/data/dva/survey_raster_gain_corr/'\n",
    "dir_in_az    = '/srv/data/dva/survey_azimuth_scans/'\n",
    "dir_out_az    = '/srv/data/dva/survey_azimuth_scans_gain_corr/'\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in files listing start and stop times for azimuth and raster scans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id = []    # The scan id number\n",
    "scan_start = []  # Start time of the scan (UTC)\n",
    "scan_stop = []   # Stop time of the scan (UTC)\n",
    "\n",
    "# Read in the azimuth scan data and store it in arrays:\n",
    "with open(dir_in_az+'DVAsurvey_phase1_day0'+day+'.txt') as fp:\n",
    "    for line in fp:       \n",
    "        scan_id.append(int(line.split()[0]))\n",
    "        scan_start.append(line.split()[1]+'T'+line.split()[2][0:12])\n",
    "        scan_stop.append(line.split()[3]+'T'+line.split()[4][0:12])\n",
    "        \n",
    "# Read in the raster scan data and store it in arrays:\n",
    "with open(dir_in_rast+'dva_survey_phase1_day0'+day+'_raster1.txt') as fp:\n",
    "    for line in fp:  \n",
    "        raster1_start = line.split()[3]\n",
    "        raster1_stop  = line.split()[4]\n",
    "with open(dir_in_rast+'dva_survey_phase1_day0'+day+'_raster2.txt') as fp:\n",
    "    for line in fp:  \n",
    "        raster2_start = line.split()[3]\n",
    "        raster2_stop  = line.split()[4]\n",
    "        \n",
    "print('raster 1:',raster1_start,raster1_stop)\n",
    "print('')\n",
    "for i in range(0,len(scan_id)):\n",
    "    print(f\"{scan_id[i]:04}\",scan_start[i],scan_stop[i])\n",
    "print('')\n",
    "print('raster 2:',raster2_start,raster2_stop)\n",
    "\n",
    "# Convert start and stop times to Modified Julian Day (MJD).\n",
    "scan_start_mjd = Time(scan_start, format='isot',scale='utc').mjd\n",
    "scan_stop_mjd  = Time(scan_stop,  format='isot',scale='utc').mjd\n",
    "raster1_start_mjd = Time(raster1_start, format='isot',scale='utc').mjd\n",
    "raster1_stop_mjd  = Time(raster1_stop,  format='isot',scale='utc').mjd\n",
    "raster2_start_mjd = Time(raster2_start, format='isot',scale='utc').mjd\n",
    "raster2_stop_mjd  = Time(raster2_stop,  format='isot',scale='utc').mjd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and stitch together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def concatenate_data(file,RR,LL,reRL,imRL,trim,dec,ra,el,az,t,noise,trim_exist):\n",
    "    \n",
    "    #print('')\n",
    "    #print(trim_exist)\n",
    "    #print('')\n",
    "    \n",
    "    dataset = file['data']['beam_0']['band_SB0']['scan_0']\n",
    "    \n",
    "    if trim_exist == True:\n",
    "        trim_flag = dataset['metadata']['trim_scan_flag']\n",
    "    else:\n",
    "        trim_flag = np.zeros_like(dataset['metadata']['noise_state'])\n",
    "    w = np.where(trim_flag == 0)[0]\n",
    "    \n",
    "    # Add the position and time data to the corresponding arrays:\n",
    "    dec = np.concatenate([dec,dataset['metadata']['declination'][w]])\n",
    "    ra = np.concatenate([ra,dataset['metadata']['right_ascension'][w]])\n",
    "    el = np.concatenate([el,dataset['metadata']['elevation'][w]])\n",
    "    az = np.concatenate([az,dataset['metadata']['azimuth'][w]])\n",
    "    t = np.concatenate([t,dataset['metadata']['utc'][w]])\n",
    "    noise = np.concatenate([noise,dataset['metadata']['noise_state'][w]])\n",
    "\n",
    "    # Add the spectrometer power data to the corresponding arrays:\n",
    "    RR = np.concatenate([RR,dataset['data'][w,0,:]],axis=0)\n",
    "    LL = np.concatenate([LL,dataset['data'][w,1,:]],axis=0)\n",
    "    reRL = np.concatenate([reRL,dataset['data'][w,2,:]],axis=0)\n",
    "    imRL = np.concatenate([imRL,dataset['data'][w,3,:]],axis=0)\n",
    "    \n",
    "    return RR,LL,reRL,imRL,trim,dec,ra,el,az,t,noise\n",
    "\n",
    "t = []\n",
    "az = []\n",
    "dec = []\n",
    "ra = []\n",
    "el = []\n",
    "noise = []\n",
    "trim_flag = []\n",
    "\n",
    "# Use one of the scans to get the list of frequencies:\n",
    "scan0 = f\"{scan_id[0]:04}\"\n",
    "file = h5py.File(dir_in_az+'dva_survey_raw_scan_'+scan0+'.h5','r')\n",
    "freq = file['data']['beam_0']['band_SB0']['frequency'][:]/1e6\n",
    "\n",
    "# Create empty arrays for the power data:\n",
    "RR = np.empty([0,len(freq)])\n",
    "LL = np.empty([0,len(freq)])\n",
    "reRL = np.empty([0,len(freq)])\n",
    "imRL = np.empty([0,len(freq)])\n",
    "\n",
    "# Raster scan 1:\n",
    "file = h5py.File(dir_in_rast+'dva_survey_phase1_day0'+day+'_raster1'+'.h5','r')\n",
    "rast1_file_t = [Time(file['data']['beam_0']['band_SB0']['scan_0']['metadata']['utc'][0],\n",
    "                    format='isot',scale='utc').mjd,\n",
    "                Time(file['data']['beam_0']['band_SB0']['scan_0']['metadata']['utc'][-1],\n",
    "                    format='isot',scale='utc').mjd]\n",
    "RR,LL,reRL,imRL,trim_flag,dec,ra,el,az,t,noise = concatenate_data(file,RR,LL,reRL,imRL,trim_flag,dec,\n",
    "                                                                  ra,el,az,t,noise,trim_rast_exist)\n",
    "\n",
    "# Loop through all the scans in the \"scan_num\" list:\n",
    "for i in scan_id:\n",
    "    file = h5py.File(dir_in_az+'dva_survey_raw_scan_'+f\"{i:04}\"+'.h5','r')\n",
    "    print(i,file)\n",
    "    RR,LL,reRL,imRL,trim_flag,dec,ra,el,az,t,noise = concatenate_data(file,RR,LL,reRL,imRL,trim_flag,dec,\n",
    "                                                                      ra,el,az,t,noise,trim_az_exist)\n",
    "    \n",
    "# Raster scan 2:\n",
    "file = h5py.File(dir_in_rast+'dva_survey_phase1_day0'+day+'_raster2'+'.h5','r')\n",
    "rast2_file_t = [Time(file['data']['beam_0']['band_SB0']['scan_0']['metadata']['utc'][0],\n",
    "                    format='isot',scale='utc').mjd,\n",
    "                Time(file['data']['beam_0']['band_SB0']['scan_0']['metadata']['utc'][-1],\n",
    "                    format='isot',scale='utc').mjd]\n",
    "RR,LL,reRL,imRL,trim_flag,dec,ra,el,az,t,noise = concatenate_data(file,RR,LL,reRL,imRL,trim_flag,dec,\n",
    "                                                                  ra,el,az,t,noise,trim_rast_exist)\n",
    "\n",
    "t_set_plt = Time(t, format='isot',scale='utc').mjd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rast1_file_t)\n",
    "print(rast2_file_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_num(month_name):\n",
    "    if month_name == 'Jan': month_num = '01'\n",
    "    if month_name == 'Feb': month_num = '02'\n",
    "    if month_name == 'Mar': month_num = '03'\n",
    "    if month_name == 'Apr': month_num = '04'\n",
    "    if month_name == 'May': month_num = '05'\n",
    "    if month_name == 'Jun': month_num = '06'\n",
    "    if month_name == 'Jul': month_num = '07'\n",
    "    if month_name == 'Aug': month_num = '08'\n",
    "    if month_name == 'Sep': month_num = '09'\n",
    "    if month_name == 'Oct': month_num = '10'\n",
    "    if month_name == 'Nov': month_num = '11'\n",
    "    if month_name == 'Dec': month_num = '12'\n",
    "    return(month_num)\n",
    "\n",
    "i = 0\n",
    "t_weath = []\n",
    "temp_C = []\n",
    "\n",
    "with open(\"/srv/data/dva/weather_survey_phase1.txt\") as fp:\n",
    "    for line in fp:\n",
    "        t_weath.append(str( line.split()[2]+'-'+month_to_num(line.split()[1])+'-'+line.split()[0]+\n",
    "                  'T'+line.split()[3]))\n",
    "        temp_C.append(line.split()[4])        \n",
    "\n",
    "temp_C = np.array(temp_C,dtype=float)\n",
    "t_weath_fix = Time(t_weath, format='isot',scale='utc')\n",
    "t_weath_plt = t_weath_fix.mjd\n",
    "\n",
    "#plt.plot(t_weath_plt,temp_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the noise source was being fired where expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs = plt.subplots(3,1,figsize=(20,8))\n",
    "\n",
    "axs[0].scatter(t_set_plt,noise,s=5)\n",
    "axs[0].set_xlim(raster1_start_mjd,raster1_stop_mjd)\n",
    "axs2 = axs[0].twinx()\n",
    "axs2.plot(t_set_plt,dec,color='k')\n",
    "#axs2.set_xlim(raster1_start_mjd,raster1_stop_mjd)\n",
    "axs2.set_ylim(7.5,17)\n",
    "\n",
    "axs[1].scatter(t_set_plt,noise,s=5)\n",
    "axs[1].set_xlim(scan_start_mjd[0],scan_stop_mjd[-1])\n",
    "axs2 = axs[1].twinx()\n",
    "axs2.scatter(t_set_plt,az,color='k',s=0.1)\n",
    "#axs2.set_xlim(scan_start_mjd[0],scan_stop_mjd[-1])\n",
    "axs2.set_ylim(-20,380)\n",
    "\n",
    "axs[2].scatter(t_set_plt,noise,s=5)\n",
    "axs[2].set_xlim(raster2_start_mjd,raster2_stop_mjd)\n",
    "axs2 = axs[2].twinx()\n",
    "axs2.plot(t_set_plt,dec,color='k')\n",
    "#axs2.set_xlim(raster1_start_mjd,raster1_stop_mjd)\n",
    "axs2.set_ylim(36,46)\n",
    "\n",
    "for i in range(0,3):\n",
    "    axs[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    axs[i].fmt_xdata = mdates.DateFormatter('%H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the gain at each noise source instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "################################################\n",
    "# The number of integrations to include on either \n",
    "# side of each noise source instance:\n",
    "n_off = 5\n",
    "################################################\n",
    "\n",
    "# Make arrays for on and off noise source\n",
    "LL_noise = np.zeros_like(LL)\n",
    "LL_off = np.zeros_like(LL)\n",
    "RR_noise = np.zeros_like(LL)\n",
    "RR_off = np.zeros_like(LL)\n",
    "t_noise = np.zeros_like(t_set_plt)\n",
    "\n",
    "wnoise = np.where(noise == 1)[0]\n",
    "for k,g in groupby(enumerate(wnoise),lambda x:x[0]-x[1]):\n",
    "\n",
    "    group = np.array(list(map(itemgetter(1),g)))\n",
    "    #print(group)\n",
    "    #print(group[1:-1])\n",
    "    middle = [group[int(np.floor((len(group)-1)/2))] ,group[int(np.ceil((len(group)-1)/2))]]\n",
    "    #print(middle)\n",
    "    offleft = [group[0]-n_off,group[0]-2]\n",
    "    offright = [group[-1]+2,group[-1]+n_off]\n",
    "    #print(offleft)\n",
    "    #print(offright)\n",
    "    #print('')\n",
    "    \n",
    "    LL_noise[middle,:] = np.nanmedian(LL[group[1:-1],:],axis=0)\n",
    "    LL_off[middle,:] = (np.nanmedian(LL[offleft,:],axis=0) + np.nanmedian(LL[offright,:],axis=0))/2.\n",
    "    \n",
    "    RR_noise[middle,:] = np.nanmedian(RR[group[1:-1],:],axis=0)\n",
    "    RR_off[middle,:] = (np.nanmedian(RR[offleft,:],axis=0) + np.nanmedian(RR[offright,:],axis=0))/2.\n",
    "    \n",
    "    t_noise[middle] = t_set_plt[middle]\n",
    "\n",
    "LL_dnoise = LL_noise - LL_off\n",
    "RR_dnoise = RR_noise - RR_off\n",
    "wnoise_pt = np.where(t_noise > 0)[0]\n",
    "GL = LL_dnoise/np.nanmean(LL_dnoise[wnoise_pt,:],axis=0)\n",
    "GR = RR_dnoise/np.nanmean(RR_dnoise[wnoise_pt,:],axis=0)\n",
    "\n",
    "wzeroL = np.where(GL == 0.)\n",
    "wzeroR = np.where(GR == 0.)\n",
    "\n",
    "GL[wzeroL] = np.nan\n",
    "GR[wzeroR] = np.nan\n",
    "\n",
    "print(LL_dnoise.shape)\n",
    "print(GL.shape)\n",
    "print(GL[wnoise_pt,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs = plt.subplots(1,1,figsize=(14,6))\n",
    "\n",
    "GL_stdev = np.nanstd(GL[wnoise_pt,:],axis=0)\n",
    "GR_stdev = np.nanstd(GR[wnoise_pt,:],axis=0)\n",
    "\n",
    "axs.scatter(freq,np.nanstd(GL[wnoise_pt,:],axis=0),s=1)\n",
    "axs.scatter(freq,np.nanstd(GR[wnoise_pt,:],axis=0),s=1)\n",
    "axs.grid()\n",
    "axs.set_ylim(0,0.2)\n",
    "axs.set_xlim(350,1030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GL_fix = GL.copy()\n",
    "GR_fix = GR.copy()\n",
    "std_mult = 1.0\n",
    "for i in range(0,len(freq)):\n",
    "    \n",
    "    print(i,freq[i])\n",
    "    \n",
    "    for j in range(0,len(wnoise_pt)):\n",
    "        \n",
    "        window = 20\n",
    "        if ((j >= window) & (j < len(wnoise_pt)-window)):\n",
    "            GL_near = np.nanmean(GL[wnoise_pt[j-window]:wnoise_pt[j+window],i])\n",
    "            GR_near = np.nanmean(GR[wnoise_pt[j-window]:wnoise_pt[j+window],i])\n",
    "        else:\n",
    "            if j < window:\n",
    "                GL_near = np.nanmean(GL[wnoise_pt[0]:wnoise_pt[j+window],i])\n",
    "                GR_near = np.nanmean(GR[wnoise_pt[0]:wnoise_pt[j+window],i])\n",
    "            if j >= len(wnoise_pt)-window:\n",
    "                GL_near = np.nanmean(GL[wnoise_pt[j-window]:wnoise_pt[-1],i])\n",
    "                GR_near = np.nanmean(GR[wnoise_pt[j-window]:wnoise_pt[-1],i])\n",
    "        \n",
    "        if ( GL[wnoise_pt[j],i] > GL_near+std_mult*GL_stdev[i]):\n",
    "            GL_fix[wnoise_pt[j],i] = np.nan\n",
    "        if ( GL[wnoise_pt[j],i] < GL_near-std_mult*GL_stdev[i]):\n",
    "            GL_fix[wnoise_pt[j],i] = np.nan\n",
    "            \n",
    "        if ( GR[wnoise_pt[j],i] > GR_near+std_mult*GR_stdev[i] ):\n",
    "            GR_fix[wnoise_pt[j],i] = np.nan\n",
    "        if ( GR[wnoise_pt[j],i] < GR_near-std_mult*GR_stdev[i] ):\n",
    "            GR_fix[wnoise_pt[j],i] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GL_fit = np.empty_like(GL)\n",
    "GR_fit = np.empty_like(GR)\n",
    "for i in range(0,len(freq)):\n",
    "#for i in range(0,500):\n",
    "    wuse = np.where(np.isfinite(GL_fix[:,i]))[0]\n",
    "    pL = np.polyfit(t_set_plt[wuse],GL_fix[wuse,i],deg=5)\n",
    "    GL_fit[:,i] = pL[0]*t_set_plt**5 + pL[1]*t_set_plt**4 + pL[2]*t_set_plt**3 + \\\n",
    "                  pL[3]*t_set_plt**2 + pL[4]*t_set_plt + pL[5]\n",
    "    print(freq[i])\n",
    "    wuse = np.where(np.isfinite(GR_fix[:,i]))[0]\n",
    "    pR = np.polyfit(t_set_plt[wuse],GR_fix[wuse,i],deg=5)\n",
    "    GR_fit[:,i] = pR[0]*t_set_plt**5 + pR[1]*t_set_plt**4 + pR[2]*t_set_plt**3 + \\\n",
    "                  pR[3]*t_set_plt**2 + pR[4]*t_set_plt + pR[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot power and gain for a single frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fplot = 500\n",
    "df = freq[1]-freq[0]\n",
    "wf = np.where(abs(freq-fplot)<df/2)[0][0]\n",
    "print(freq[wf])\n",
    "sz=1\n",
    "fig1, axs = plt.subplots(3,1,figsize=(20,12))\n",
    "\n",
    "for i in range(0,3):\n",
    "    axs[i].scatter(t_set_plt,LL[:,wf],s=sz)\n",
    "    axs[i].scatter(t_set_plt,LL[:,wf]/GL_fit[:,wf],s=sz,color='red')\n",
    "    axs[i].scatter(t_set_plt,LL_noise[:,wf],s=sz*5)\n",
    "    axs[i].scatter(t_set_plt,LL_off[:,wf],s=sz*5,color='k')\n",
    "    ax2 = axs[i].twinx()\n",
    "    ax2.scatter(t_set_plt,GL[:,wf],s=20,color='blue')\n",
    "    ax2.scatter(t_set_plt,GL_fix[:,wf],s=5,color='red')\n",
    "    ax2.plot(t_set_plt,GL_fit[:,wf],color='k')\n",
    "    ax2.set_ylim(0.8,1.2)\n",
    "    axs[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    axs[i].fmt_xdata = mdates.DateFormatter('%H:%M:%S')\n",
    "    axs[i].grid()\n",
    "\n",
    "axs[0].set_xlim(raster1_start_mjd,raster1_stop_mjd)\n",
    "#axs[0].set_xlim(raster1_start_mjd,raster1_start_mjd+0.005)\n",
    "axs[0].set_ylim(2.8e7,4e7)\n",
    "axs[1].set_xlim(scan_start_mjd[0],scan_stop_mjd[-1])\n",
    "axs[1].set_ylim(3e7,9e7)\n",
    "axs[2].set_xlim(raster2_start_mjd,raster2_stop_mjd)\n",
    "axs[2].set_ylim(3e7,10.5e7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs = plt.subplots(1,1,figsize=(20,4))\n",
    "\n",
    "axs.scatter(t_set_plt,GL[:,wf],s=20,color='blue')\n",
    "axs.scatter(t_set_plt,GL_fix[:,wf],s=20,color='red')\n",
    "axs.plot(t_set_plt,GL_fit[:,wf],color='black')\n",
    "\n",
    "axs.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "axs.fmt_xdata = mdates.DateFormatter('%H:%M:%S')\n",
    "axs.grid()\n",
    "axs.set_ylim(0.8,1.2)\n",
    "ax2 = axs.twinx()\n",
    "ax2.plot(t_weath_plt,temp_C)\n",
    "ax2.set_ylim(5,20)\n",
    "axs.set_xlim(raster1_start_mjd,raster2_stop_mjd)\n",
    "#axs.set_xlim(raster1_start_mjd,raster1_start_mjd+0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waterfall plot for power, gain and temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Gmin = 0.9\n",
    "Gmax = 1.1\n",
    "dBmin = 67\n",
    "dBmax = 75\n",
    "\n",
    "fplot = 800\n",
    "df = freq[1]-freq[0]\n",
    "wf = np.where(abs(freq-fplot)<df/2)[0][0]\n",
    "print(freq[wf])\n",
    "\n",
    "fig1, axs = plt.subplots(3,1,figsize=(20,18),sharex=True,gridspec_kw={'height_ratios': [1,1,0.5]},\n",
    "                         constrained_layout=True)\n",
    "fs = 16\n",
    "\n",
    "for i in range(0,len(scan_id)):\n",
    "    w = np.where((t_set_plt>=scan_start_mjd[i]) & (t_set_plt<=scan_stop_mjd[i]))[0]\n",
    "    extent = [scan_start_mjd[i],scan_stop_mjd[i],freq[0],freq[-1]]\n",
    "    \n",
    "    im1 = axs[0].imshow(10*np.log10(LL[w,:].T),aspect='auto',vmin=dBmin,vmax=dBmax,origin='lower',\n",
    "                        extent=extent,cmap='viridis')    \n",
    "    im2 = axs[1].imshow(GL_fit[w,:].T,aspect='auto',vmin=Gmin,vmax=Gmax,origin='lower',\n",
    "                        extent=extent,cmap='RdBu')\n",
    "    \n",
    "w = np.where((t_set_plt>=raster1_start_mjd) & (t_set_plt<=raster1_stop_mjd))[0]\n",
    "extent = [raster1_start_mjd,raster1_stop_mjd,freq[0],freq[-1]]\n",
    "\n",
    "im1 = axs[0].imshow(10*np.log10(LL[w,:].T),aspect='auto',vmin=dBmin,vmax=dBmax,origin='lower',\n",
    "                    extent=extent,cmap='viridis')\n",
    "im2 = axs[1].imshow(GL_fit[w,:].T,aspect='auto',vmin=Gmin,vmax=Gmax,origin='lower',\n",
    "                    extent=extent,cmap='RdBu')\n",
    "\n",
    "w = np.where((t_set_plt>=raster2_start_mjd) & (t_set_plt<=raster2_stop_mjd))[0]\n",
    "extent = [raster2_start_mjd,raster2_stop_mjd,freq[0],freq[-1]]\n",
    "\n",
    "im1 = axs[0].imshow(10*np.log10(LL[w,:].T),aspect='auto',vmin=dBmin,vmax=dBmax,origin='lower',\n",
    "                    extent=extent,cmap='viridis')\n",
    "im2 = axs[1].imshow(GL_fit[w,:].T,aspect='auto',vmin=Gmin,vmax=Gmax,origin='lower',\n",
    "                    extent=extent,cmap='RdBu')\n",
    "\n",
    "cbar1= fig1.colorbar(im1,ax=axs[0])\n",
    "cbar1.ax.tick_params(labelsize=fs) \n",
    "cbar1.set_label('Power (dB)', fontsize=fs)\n",
    "\n",
    "cbar2 = fig1.colorbar(im2,ax=axs[1])\n",
    "cbar2.ax.tick_params(labelsize=fs) \n",
    "cbar2.set_label('Gain', fontsize=fs)\n",
    "\n",
    "axs[2].plot(t_weath_plt,temp_C,color='purple',linewidth=3)\n",
    "axs[2].set_ylim(-5,35)     \n",
    "axs[2].set_ylabel('Temperature (C)',fontsize=fs)\n",
    "axs[2].axvspan(raster1_start_mjd,raster1_stop_mjd,color='C0',alpha=0.1)\n",
    "axs[2].axvspan(raster2_start_mjd,raster2_stop_mjd,color='C0',alpha=0.1)\n",
    "ax2 = axs[2].twinx()\n",
    "ax2.scatter(t_set_plt,GL[:,wf],s=10,color='red')\n",
    "ax2.scatter(t_set_plt,GL_fix[:,wf],s=10,color='k')\n",
    "ax2.set_ylim(0.5,1.5)\n",
    "ax2.set_ylabel('Gain at 800 MHz',fontsize=fs)\n",
    "ax2.tick_params(axis='y', labelsize=fs)\n",
    "for i in range(0,len(scan_id)):\n",
    "    elhere = el[abs(t_set_plt-scan_start_mjd[i])<1e-4][0]\n",
    "    if abs(elhere-49.32)<0.5:\n",
    "        clr = 'C1'\n",
    "    elif abs(elhere-20.0)<0.5:\n",
    "        clr = 'C2'\n",
    "    axs[2].axvspan(scan_start_mjd[i],scan_stop_mjd[i],color=clr,alpha=0.1)\n",
    "axs[2].grid() \n",
    "\n",
    "for i in range(0,3):\n",
    "    axs[i].set_xlim(t_set_plt[0],t_set_plt[-1])\n",
    "    axs[i].tick_params(axis='both', labelsize=fs,labelbottom=True)\n",
    "    axs[i].xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "    axs[i].fmt_xdata = mdates.DateFormatter('%H:%M:%S')\n",
    "    axs[i].set_xlabel('Time (UTC)',fontsize=fs)        \n",
    "for i in range(0,2):\n",
    "    axs[i].set_ylim(freq[0],freq[-1])\n",
    "    axs[i].set_ylabel('Frequency (MHz)',fontsize=fs)\n",
    "    \n",
    "#plt.tight_layout()\n",
    "plt.savefig('../DVA_PLOTS/gains_waterfall_phase1_day'+day+'_v2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking frequency dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = [100,6000]\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(20,10))\n",
    "\n",
    "print(t[0])\n",
    "print(t[10000])\n",
    "\n",
    "axs.plot(freq,LL[0,:])\n",
    "axs.plot(freq,LL[0,:]/GL_fit[0,:]-1.6e5,color='red',linestyle='dashed')\n",
    "axs.plot(freq,LL[0,:]/GL_fit[0,:]-7.5e5,color='blue',linestyle='dashed')\n",
    "axs.plot(freq,LL[10000,:])\n",
    "ax2 = axs.twinx()\n",
    "ax2.scatter(freq,GL_fit[0,:],color='black',s=5)\n",
    "ax2.scatter(freq,GL_fit[10000,:],color='black',s=5)\n",
    "ax2.set_ylim(0.85,1.15)\n",
    "\n",
    "axs.set_ylim(0.3e7,5.0e7)\n",
    "axs.set_xlim(350,1030)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out gain-corrected files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gain_corrected(file,RR_out,LL_out,reRL_out,imRL_out,infiles,outfiles,inname,outname,GL_use,GR_use):\n",
    "\n",
    "    cmd2 = 'cp '+infiles+inname+' '+outfiles+outname+'.h5'\n",
    "    os.system(cmd2)\n",
    "    file_new = h5py.File(outfiles+outname+'.h5','r+')\n",
    "    \n",
    "    file_new['data']['beam_0']['band_SB0']['scan_0']['data'][:,0,:] = RR_out/GR_use\n",
    "    file_new['data']['beam_0']['band_SB0']['scan_0']['data'][:,1,:] = LL_out/GL_use\n",
    "    file_new['data']['beam_0']['band_SB0']['scan_0']['data'][:,2,:] = reRL_out/np.sqrt(abs(GR_use*GL_use))\n",
    "    file_new['data']['beam_0']['band_SB0']['scan_0']['data'][:,3,:] = imRL_out/np.sqrt(abs(GR_use*GL_use))\n",
    "           \n",
    "    file_new.close()\n",
    "\n",
    "    \n",
    "# Raster scan 1:\n",
    "inname  = 'dva_survey_phase1_day0'+day+'_raster1'+'.h5'\n",
    "outname = 'dva_survey_phase1_day0'+day+'_raster1_gain_corr'\n",
    "file = h5py.File(dir_in_rast+inname,'r')\n",
    "w = np.where((t_set_plt >= rast1_file_t[0]) & (t_set_plt <= rast1_file_t[-1]))[0]\n",
    "\n",
    "write_gain_corrected(file,RR[w,:],LL[w,:],reRL[w,:],imRL[w,:],dir_in_rast,dir_out_rast,inname,outname,\n",
    "                     GL_fit[w],GR_fit[w])\n",
    "\n",
    "# Raster scan 2:\n",
    "inname  = 'dva_survey_phase1_day0'+day+'_raster2'+'.h5'\n",
    "outname = 'dva_survey_phase1_day0'+day+'_raster2_gain_corr'\n",
    "file = h5py.File(dir_in_rast+inname,'r')\n",
    "w = np.where((t_set_plt >= rast2_file_t[0]) & (t_set_plt <= rast2_file_t[-1]))[0]\n",
    "write_gain_corrected(file,RR[w,:],LL[w,:],reRL[w,:],imRL[w,:],dir_in_rast,dir_out_rast,inname,outname,\n",
    "                     GL_fit[w],GR_fit[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test new file by comparing to old file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test = h5py.File(dir_out_rast+'dva_survey_phase1_day0'+day+'_raster1_gain_corr.h5','r')\n",
    "dataset_test = file_test['data']['beam_0']['band_SB0']['scan_0']\n",
    " \n",
    "fplot = 800\n",
    "df = freq[1]-freq[0]\n",
    "wf = np.where(abs(freq-fplot)<df/2)[0][0]\n",
    "print(freq[wf])    \n",
    "    \n",
    "dec_test = dataset_test['metadata']['declination']\n",
    "ra_test = dataset_test['metadata']['right_ascension']\n",
    "el_test = dataset_test['metadata']['elevation']\n",
    "az_test = dataset_test['metadata']['azimuth']\n",
    "t_test = dataset_test['metadata']['utc']\n",
    "noise_test = dataset_test['metadata']['noise_state']\n",
    "\n",
    "RR_test = dataset_test['data'][:,0,:]\n",
    "LL_test = dataset_test['data'][:,1,:]\n",
    "reRL_test = dataset_test['data'][:,2,:]\n",
    "imRL_test = dataset_test['data'][:,3,:]\n",
    "    \n",
    "t_test_plt = Time(t_test, format='isot',scale='utc').mjd\n",
    "\n",
    "fig1, axs = plt.subplots(2,1,figsize=(20,10))\n",
    "\n",
    "axs[0].plot(t_set_plt,dec,color='C2')\n",
    "axs[0].plot(t_test_plt,dec_test,color='black',linestyle='dashed')\n",
    "axs[0].set_xlim(t_test_plt[0],t_test_plt[-1])\n",
    "axs[0].set_ylim(0,20)\n",
    "\n",
    "axs[1].plot(t_set_plt,LL[:,wf],color='C0')\n",
    "axs[1].plot(t_test_plt,LL_test[:,wf],color='blue',linestyle='dashed')\n",
    "axs[1].plot(t_set_plt,RR[:,wf],color='C1')\n",
    "axs[1].plot(t_test_plt,RR_test[:,wf],color='red',linestyle='dashed')\n",
    "axs[1].set_xlim(t_test_plt[0],t_test_plt[-1])\n",
    "axs[1].set_ylim(0.5e7,2e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test = h5py.File(dir_out_rast+'dva_survey_phase1_day0'+day+'_raster2_gain_corr.h5','r')\n",
    "dataset_test = file_test['data']['beam_0']['band_SB0']['scan_0']\n",
    " \n",
    "fplot = 800\n",
    "df = freq[1]-freq[0]\n",
    "wf = np.where(abs(freq-fplot)<df/2)[0][0]\n",
    "print(freq[wf])    \n",
    "    \n",
    "dec_test = dataset_test['metadata']['declination']\n",
    "ra_test = dataset_test['metadata']['right_ascension']\n",
    "el_test = dataset_test['metadata']['elevation']\n",
    "az_test = dataset_test['metadata']['azimuth']\n",
    "t_test = dataset_test['metadata']['utc']\n",
    "noise_test = dataset_test['metadata']['noise_state']\n",
    "\n",
    "RR_test = dataset_test['data'][:,0,:]\n",
    "LL_test = dataset_test['data'][:,1,:]\n",
    "reRL_test = dataset_test['data'][:,2,:]\n",
    "imRL_test = dataset_test['data'][:,3,:]\n",
    "    \n",
    "t_test_plt = Time(t_test, format='isot',scale='utc').mjd\n",
    "\n",
    "fig1, axs = plt.subplots(2,1,figsize=(20,10))\n",
    "\n",
    "axs[0].plot(t_set_plt,dec,color='C2')\n",
    "axs[0].plot(t_test_plt,dec_test,color='black',linestyle='dashed')\n",
    "axs[0].set_xlim(t_test_plt[0],t_test_plt[-1])\n",
    "axs[0].set_ylim(20,60)\n",
    "\n",
    "axs[1].plot(t_set_plt,LL[:,wf],color='C0')\n",
    "axs[1].plot(t_test_plt,LL_test[:,wf],color='blue',linestyle='dashed')\n",
    "axs[1].plot(t_set_plt,RR[:,wf],color='C1')\n",
    "axs[1].plot(t_test_plt,RR_test[:,wf],color='red',linestyle='dashed')\n",
    "axs[1].set_xlim(t_test_plt[0],t_test_plt[-1])\n",
    "axs[1].set_ylim(0.5e7,3.5e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
